---
AWSTemplateFormatVersion: '2010-09-09'
Description: 'Kubernetes AWS CloudFormation Template 2 of 2: Create a Kubernetes Node Group.'
Metadata: 
  AWS::CloudFormation::Interface: 
    ParameterGroups: 
      - 
        Label: 
          default: "EKS Configuration"
        Parameters: 
          - VPCStackName
          - KeyName
          - NodeImageId
          - ClusterName
          - NodeGroupName
          - MasterEndpoint
          - Subnet01Block
          - Subnet02Block
      - 
        Label: 
          default: "Spot or AutoScaling Group"
        Parameters: 
          - AsgOrSpotFleet
      - 
        Label: 
          default: "Auto Scaling Configuration(Ingore this if you select 'SpotFleet' for 'AsgOrSpotFleet')"
        Parameters: 
          - OnDemandOrSpotWithASG
          - NodeAutoScalingGroupMinSize
          - NodeAutoScalingGroupMaxSize
          - NodeInstanceType
      - 
        Label: 
          default: "Spot Configuration(Ingore this if you select 'AutoscalingGroup' for 'AsgOrSpotFleet')"
        Parameters: 
          - SpotPrice
          - SpotFleetInstancesType
          - SpotFleetAllocationStrategy

    # ParameterLabels: 
    #   VPCID: 
    #     default: "Which VPC should this be deployed to?"


Parameters:
  VPCStackName:
    Description: "StackName of Kubernetes AWS CFN Template 1 of 2. Allows for importing of values from first stack."
    Type: "String"
    MinLength: 1
    MaxLength: 255
    AllowedPattern : "^[a-zA-Z][-a-zA-Z0-9]*$"

  KeyName:
    Description: The EC2 Key Pair to allow SSH access to the instances
    Type: AWS::EC2::KeyPair::KeyName

  NodeImageId:
    Type: AWS::EC2::Image::Id
    Default: "ami-74128f0c"
    Description: AMI id for the node instances.

  NodeInstanceType:
    Description: EC2 instance type for the node instances.
    Type: String
    Default: t2.medium
    AllowedValues:
    - t2.medium
    - t2.large
    - m3.medium
    - m3.large
    - m3.xlarge
    - m3.2xlarge
    - m4.large
    - m4.xlarge
    - m4.2xlarge
    - m4.4xlarge
    - m4.10xlarge
    - c3.large
    - c3.xlarge
    - c3.2xlarge
    - c3.4xlarge
    - c3.8xlarge
    - c4.large
    - c4.xlarge
    - c4.2xlarge
    - c4.4xlarge
    - c4.8xlarge
    ConstraintDescription: must be a valid EC2 instance type.

  NodeAutoScalingGroupMinSize:
    Type: Number
    Description: Minimum size of Node Group ASG.
    Default: 1

  NodeAutoScalingGroupMaxSize:
    Type: Number
    Description: Maximum size of Node Group ASG.
    Default: 3

  ClusterName:
    Description: Unique identifier for the cluster.
    Type: String

  NodeGroupName:
    Description: Unique identifier for the Node Group.
    Type: String

  MasterEndpoint:
    Description: URL endpoint of the API Server.
    Type: String

  Subnet01Block:
    Type: String
    Default: 10.0.96.0/19
    Description: CidrBlock for subnet 01

  Subnet02Block:
    Type: String
    Default: 10.0.128.0/19
    Description: CidrBlock for subnet 02

  AsgOrSpotFleet:
    Type: String
    Default: "AutoscalingGroup"
    Description: "provision the ECS cluster with AutoscalingGroup or SpotFleet"
    AllowedValues:
      - "AutoscalingGroup"
      - "SpotFleet"

  OnDemandOrSpotWithASG:
    Type: String
    Default: "On Demand"
    Description: "on demand or spot instances with AutoScaling Group"
    AllowedValues:
      - "On Demand"
      - "Spot"

  SpotPrice:
    Type: Number
    Description: Set your max price (per instance/hour)
    Default: 0  

  SpotFleetInstancesType:
    Type: CommaDelimitedList
    Description: "instance types in Spot Fleet(specify 3 instance types)"
    Default: "t2.small,t2.medium, m3.medium"

  SpotFleetAllocationStrategy:
    Type: String
    Description: "spot fleet allocation strategy"
    AllowedValues:
      - "diversified"
      - "lowestPrice"
    Default: "diversified"





Conditions:
  # SpotOnlyCond: !Not [ !Equals [ !Ref SpotPrice, 0 ] ]
  IsSetSpotPrice: !Not [ !Equals [ !Ref SpotPrice, 0 ] ]
  SpotWithASG: !Equals [ !Ref OnDemandOrSpotWithASG , "Spot" ]
  SpotFleetCond: !Equals [ !Ref AsgOrSpotFleet, "SpotFleet" ]
  EnableASGCond: !Equals [ !Ref AsgOrSpotFleet, "AutoscalingGroup" ]
  DisableASGCond: !Equals [ !Ref AsgOrSpotFleet, "SpotFleet" ]


Resources:
  Subnet01:
    Type: AWS::EC2::Subnet
    Metadata:
      Comment: Subnet 01
    Properties:
      AvailabilityZone:
        Fn::Select:
        - '0'
        - Fn::GetAZs:
            Ref: AWS::Region
      CidrBlock:
        Ref: Subnet01Block
      VpcId:
        'Fn::ImportValue': !Sub "${VPCStackName}-VPCID"
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-Subnet01"
      - Key: KubernetesCluster
        Value: !Ref ClusterName
        # Also tag it with kubernetes.io/cluster/clustername=owned, which is the newer convention for cluster resources
      - Key: !Sub 'kubernetes.io/cluster/${ClusterName}'
        Value: 'owned'

  Subnet02:
    Type: AWS::EC2::Subnet
    Metadata:
      Comment: Subnet 02
    Properties:
      AvailabilityZone:
        Fn::Select:
        - '1'
        - Fn::GetAZs:
            Ref: AWS::Region
      CidrBlock:
        Ref: Subnet02Block
      VpcId:
        'Fn::ImportValue': !Sub "${VPCStackName}-VPCID"
      Tags:
      - Key: Name
        Value: !Sub "${AWS::StackName}-Subnet02"
      - Key: KubernetesCluster
        Value: !Ref ClusterName
        # Also tag it with kubernetes.io/cluster/clustername=owned, which is the newer convention for cluster resources
      - Key: !Sub 'kubernetes.io/cluster/${ClusterName}'
        Value: 'owned'

  Subnet01RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref Subnet01
      RouteTableId:
        'Fn::ImportValue': !Sub "${VPCStackName}-RouteTable"

  Subnet02RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref Subnet02
      RouteTableId:
        'Fn::ImportValue': !Sub "${VPCStackName}-RouteTable"

  NodeInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: "/"
      Roles:
      - !Ref NodeInstanceRole

  NodeInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - ec2.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: "/"
      Policies:
      - PolicyName: node
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - ec2:Describe*
            - ecr:GetAuthorizationToken
            - ecr:BatchCheckLayerAvailability
            - ecr:GetDownloadUrlForLayer
            - ecr:GetRepositoryPolicy
            - ecr:DescribeRepositories
            - ecr:ListImages
            - ecr:BatchGetImage
            Resource: "*"
      - PolicyName: cni
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - ec2:CreateNetworkInterface
            - ec2:AttachNetworkInterface
            - ec2:DeleteNetworkInterface
            - ec2:DetachNetworkInterface
            - ec2:ModifyNetworkInterfaceAttribute
            - ec2:AssignPrivateIpAddresses
            - tag:TagResources
            Resource: "*"

  NodeSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for all nodes in the cluster
      VpcId:
        'Fn::ImportValue': !Sub "${VPCStackName}-VPCID"
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: '22'
        ToPort: '22'
        CidrIp: 0.0.0.0/0
      Tags:
      - Key: KubernetesCluster
        Value: !Ref ClusterName
      - Key: !Sub "kubernetes.io/cluster/${ClusterName}"
        Value: 'owned'

  NodeSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId:
        Ref: NodeSecurityGroup
      IpProtocol: '-1'
      ToPort: '65535'
      FromPort: '0'
      SourceSecurityGroupId:
        Ref: NodeSecurityGroup
    DependsOn: NodeSecurityGroup

  NodeGroup:
    Condition: EnableASGCond
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      DesiredCapacity: !Ref NodeAutoScalingGroupMaxSize
      LaunchConfigurationName: !Ref NodeLaunchConfig
      MinSize: !Ref NodeAutoScalingGroupMinSize
      MaxSize: !Ref NodeAutoScalingGroupMaxSize
      VPCZoneIdentifier:
        - !Ref Subnet01
        - !Ref Subnet02
      Tags:
      - Key: Name
        Value: !Sub "${ClusterName}-${NodeGroupName}-Node"
        PropagateAtLaunch: 'true'
      - Key: KubernetesCluster
        Value: !Ref ClusterName
        PropagateAtLaunch: 'true'
        # Also tag it with kubernetes.io/cluster/clustername=owned, which is the newer convention for cluster resources
      - Key: !Sub 'kubernetes.io/cluster/${ClusterName}'
        Value: 'owned'
        PropagateAtLaunch: 'true'
    UpdatePolicy:
      AutoScalingRollingUpdate:
        MinInstancesInService: '0'
        MaxBatchSize: '1'

  # TODO: Make disk configurable
  NodeLaunchConfig:
    Condition: EnableASGCond
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: 'true'
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: !Ref NodeImageId
      InstanceType: !Ref NodeInstanceType
      KeyName: !Ref KeyName
      SecurityGroups:
      - !Ref NodeSecurityGroup
      SpotPrice: 
        !If 
          - SpotWithASG
          - !Ref SpotPrice
          - !Ref AWS::NoValue    
      UserData:
        Fn::Base64:
          Fn::Join: [
            "",
            [
              "#!/bin/bash -x\n",
              "INTERNAL_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)", "\n",
              "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /var/lib/kubelet/kubeconfig", "\n",
              "sed -i s,CLUSTER_NAME,", { Ref: ClusterName }, ",g /var/lib/kubelet/kubeconfig", "\n",
              "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kubelet.service", "\n",
              "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kube-proxy.service", "\n",
              "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kubelet.service", "\n",
              "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kube-proxy.service", "\n",
              "systemctl daemon-reload", "\n",
              "systemctl restart kubelet kube-proxy", "\n",
              "echo 'start' > /tmp/userdata.log", "\n",
              "restart_if_fail() { sleep 10; journalctl -u kubelet --since '10 seconds ago' -r -n1 | egrep '[E|W]0401' >> /tmp/userdata.log && echo 'got failure, restarting kubelet and kube-proxy now' | tee -a /tmp/userdata.log && systemctl restart kubelet kube-proxy; }", "\n",
              "for i in 1 2 3 4 5; do restart_if_fail; done", "\n",
            ]
          ]


###################
# Spot fleet
###################

  IAMFleetRole:
    Type: AWS::IAM::Role
    Properties:
        AssumeRolePolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Action: 
                  - 'sts:AssumeRole'
                Effect: Allow
                Principal:
                  Service:
                    - spotfleet.amazonaws.com
        Path: /
        ManagedPolicyArns:
            - arn:aws:iam::aws:policy/service-role/AmazonEC2SpotFleetRole   
            - arn:aws:iam::aws:policy/service-role/AmazonEC2SpotFleetTaggingRole

  SpotFleet:
    Condition: SpotFleetCond
    Type: AWS::EC2::SpotFleet
    Properties:
      SpotFleetRequestConfigData:
        AllocationStrategy: !Ref SpotFleetAllocationStrategy
        # SpotPrice: '0.02'
        SpotPrice: 
          !If 
            - IsSetSpotPrice
            - !Ref SpotPrice
            - !Ref AWS::NoValue  
        IamFleetRole: !GetAtt IAMFleetRole.Arn
        TargetCapacity: !Ref NodeAutoScalingGroupMaxSize
        LaunchSpecifications:
        - EbsOptimized: 'false'
          WeightedCapacity: 1
          IamInstanceProfile: 
            Arn: !GetAtt NodeInstanceProfile.Arn
          InstanceType: !Select [0, !Ref SpotFleetInstancesType]
          # ImageId: !GetAtt AMIInfo.Id
          ImageId: !Ref NodeImageId
          TagSpecifications:
          -
            ResourceType: instance
            Tags:
              - Key: Name
                Value: !Sub "${ClusterName}-${NodeGroupName}-Node"
                # PropagateAtLaunch: 'true'
              - Key: KubernetesCluster
                Value: !Ref ClusterName
                # PropagateAtLaunch: 'true'
                # Also tag it with kubernetes.io/cluster/clustername=owned, which is the newer convention for cluster resources
              - Key: !Sub 'kubernetes.io/cluster/${ClusterName}'
                Value: 'owned'
                # PropagateAtLaunch: 'true'            
          KeyName: !Ref KeyName
            # !If 
            #   - HasSshKeyName
            #   - !Ref SshKeyName
            #   - !Ref AWS::NoValue
          Monitoring:
            Enabled: 'true'
          NetworkInterfaces:
            - AssociatePublicIpAddress: true
              DeviceIndex: 0
              SubnetId: !Ref Subnet01
              Groups:
                - !Ref NodeSecurityGroup
          UserData: 
            Fn::Base64:
              Fn::Join: [
                "",
                [
                  "#!/bin/bash -x\n",
                  "INTERNAL_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /var/lib/kubelet/kubeconfig", "\n",
                  "sed -i s,CLUSTER_NAME,", { Ref: ClusterName }, ",g /var/lib/kubelet/kubeconfig", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kubelet.service", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kube-proxy.service", "\n",
                  "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kubelet.service", "\n",
                  "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kube-proxy.service", "\n",
                  "systemctl daemon-reload", "\n",
                  "systemctl restart kubelet kube-proxy", "\n",
                  "echo 'start' > /tmp/userdata.log", "\n",
                  "restart_if_fail() { sleep 10; journalctl -u kubelet --since '10 seconds ago' -r -n1 | egrep '[E|W]0401' >> /tmp/userdata.log && echo 'got failure, restarting kubelet and kube-proxy now' | tee -a /tmp/userdata.log && systemctl restart kubelet kube-proxy; }", "\n",
                  "for i in 1 2 3 4 5; do restart_if_fail; done", "\n",                  

                ]
              ]
        - EbsOptimized: 'false'
          WeightedCapacity: 1
          IamInstanceProfile: 
             Arn: !GetAtt NodeInstanceProfile.Arn
          InstanceType: !Select [0, !Ref SpotFleetInstancesType]
          # ImageId: !GetAtt AMIInfo.Id
          ImageId: !Ref NodeImageId
          TagSpecifications:
          -
            ResourceType: instance
            Tags:
              - Key: Name
                Value: !Sub "${ClusterName}-${NodeGroupName}-Node"
                # PropagateAtLaunch: 'true'
              - Key: KubernetesCluster
                Value: !Ref ClusterName
                # PropagateAtLaunch: 'true'
                # Also tag it with kubernetes.io/cluster/clustername=owned, which is the newer convention for cluster resources
              - Key: !Sub 'kubernetes.io/cluster/${ClusterName}'
                Value: 'owned'
                # PropagateAtLaunch: 'true'     
          KeyName: !Ref KeyName
            # !If 
            #   - HasSshKeyName
            #   - !Ref SshKeyName
            #   - !Ref AWS::NoValue
          Monitoring:
            Enabled: 'true'
          NetworkInterfaces:
            - AssociatePublicIpAddress: true
              DeviceIndex: 0
              SubnetId: !Ref Subnet02
              Groups:
                - !Ref NodeSecurityGroup
          UserData: 
            Fn::Base64:
              Fn::Join: [
                "",
                [
                  "#!/bin/bash -x\n",
                  "INTERNAL_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /var/lib/kubelet/kubeconfig", "\n",
                  "sed -i s,CLUSTER_NAME,", { Ref: ClusterName }, ",g /var/lib/kubelet/kubeconfig", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kubelet.service", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kube-proxy.service", "\n",
                  "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kubelet.service", "\n",
                  "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kube-proxy.service", "\n",
                  "systemctl daemon-reload", "\n",
                  "systemctl restart kubelet kube-proxy", "\n",
                  "echo 'start' > /tmp/userdata.log", "\n",
                  "restart_if_fail() { sleep 10; journalctl -u kubelet --since '10 seconds ago' -r -n1 | egrep '[E|W]0401' >> /tmp/userdata.log && echo 'got failure, restarting kubelet and kube-proxy now' | tee -a /tmp/userdata.log && systemctl restart kubelet kube-proxy; }", "\n",
                  "for i in 1 2 3 4 5; do restart_if_fail; done", "\n",                     
                ]
              ]
        - EbsOptimized: 'false'
          WeightedCapacity: 1
          IamInstanceProfile: 
             Arn: !GetAtt NodeInstanceProfile.Arn
          InstanceType: !Select [1, !Ref SpotFleetInstancesType]
          # ImageId: !GetAtt AMIInfo.Id
          ImageId: !Ref NodeImageId
          TagSpecifications:
          -
            ResourceType: instance
            Tags:
              - Key: Name
                Value: !Sub "${ClusterName}-${NodeGroupName}-Node"
                # PropagateAtLaunch: 'true'
              - Key: KubernetesCluster
                Value: !Ref ClusterName
                # PropagateAtLaunch: 'true'
                # Also tag it with kubernetes.io/cluster/clustername=owned, which is the newer convention for cluster resources
              - Key: !Sub 'kubernetes.io/cluster/${ClusterName}'
                Value: 'owned'
                # PropagateAtLaunch: 'true'     
          KeyName: !Ref KeyName
            # !If 
            #   - HasSshKeyName
            #   - !Ref SshKeyName
            #   - !Ref AWS::NoValue
          Monitoring:
            Enabled: 'true'
          NetworkInterfaces:
            - AssociatePublicIpAddress: true
              DeviceIndex: 0
              SubnetId: !Ref Subnet01
              Groups:
                - !Ref NodeSecurityGroup
          UserData: 
            Fn::Base64:
              Fn::Join: [
                "",
                [
                  "#!/bin/bash -x\n",
                  "INTERNAL_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /var/lib/kubelet/kubeconfig", "\n",
                  "sed -i s,CLUSTER_NAME,", { Ref: ClusterName }, ",g /var/lib/kubelet/kubeconfig", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kubelet.service", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kube-proxy.service", "\n",
                  "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kubelet.service", "\n",
                  "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kube-proxy.service", "\n",
                  "systemctl daemon-reload", "\n",
                  "systemctl restart kubelet kube-proxy", "\n",
                  "echo 'start' > /tmp/userdata.log", "\n",
                  "restart_if_fail() { sleep 10; journalctl -u kubelet --since '10 seconds ago' -r -n1 | egrep '[E|W]0401' >> /tmp/userdata.log && echo 'got failure, restarting kubelet and kube-proxy now' | tee -a /tmp/userdata.log && systemctl restart kubelet kube-proxy; }", "\n",
                  "for i in 1 2 3 4 5; do restart_if_fail; done", "\n",                   
                ]
              ]
        - EbsOptimized: 'false'
          WeightedCapacity: 1
          IamInstanceProfile: 
             Arn: !GetAtt NodeInstanceProfile.Arn
          InstanceType: !Select [1, !Ref SpotFleetInstancesType]
          # ImageId: !GetAtt AMIInfo.Id
          ImageId: !Ref NodeImageId
          TagSpecifications:
          -
            ResourceType: instance
            Tags:
              - Key: Name
                Value: !Sub "${ClusterName}-${NodeGroupName}-Node"
                # PropagateAtLaunch: 'true'
              - Key: KubernetesCluster
                Value: !Ref ClusterName
                # PropagateAtLaunch: 'true'
                # Also tag it with kubernetes.io/cluster/clustername=owned, which is the newer convention for cluster resources
              - Key: !Sub 'kubernetes.io/cluster/${ClusterName}'
                Value: 'owned'
                # PropagateAtLaunch: 'true'     
          KeyName: !Ref KeyName
            # !If 
            #   - HasSshKeyName
            #   - !Ref SshKeyName
            #   - !Ref AWS::NoValue
          Monitoring:
            Enabled: 'true'
          NetworkInterfaces:
            - AssociatePublicIpAddress: true
              DeviceIndex: 0
              SubnetId: !Ref Subnet02
              Groups:
                - !Ref NodeSecurityGroup
          UserData: 
            Fn::Base64:
              Fn::Join: [
                "",
                [
                  "#!/bin/bash -x\n",
                  "INTERNAL_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /var/lib/kubelet/kubeconfig", "\n",
                  "sed -i s,CLUSTER_NAME,", { Ref: ClusterName }, ",g /var/lib/kubelet/kubeconfig", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kubelet.service", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kube-proxy.service", "\n",
                  "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kubelet.service", "\n",
                  "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kube-proxy.service", "\n",
                  "systemctl daemon-reload", "\n",
                  "systemctl restart kubelet kube-proxy", "\n",
                  "echo 'start' > /tmp/userdata.log", "\n",
                  "restart_if_fail() { sleep 10; journalctl -u kubelet --since '10 seconds ago' -r -n1 | egrep '[E|W]0401' >> /tmp/userdata.log && echo 'got failure, restarting kubelet and kube-proxy now' | tee -a /tmp/userdata.log && systemctl restart kubelet kube-proxy; }", "\n",
                  "for i in 1 2 3 4 5; do restart_if_fail; done", "\n",                   
                ]
              ]
        - EbsOptimized: 'false'
          WeightedCapacity: 1
          IamInstanceProfile: 
             Arn: !GetAtt NodeInstanceProfile.Arn
          InstanceType: !Select [2, !Ref SpotFleetInstancesType]
          # ImageId: !GetAtt AMIInfo.Id
          ImageId: !Ref NodeImageId
          TagSpecifications:
          -
            ResourceType: instance
            Tags:
              - Key: Name
                Value: !Sub "${ClusterName}-${NodeGroupName}-Node"
                # PropagateAtLaunch: 'true'
              - Key: KubernetesCluster
                Value: !Ref ClusterName
                # PropagateAtLaunch: 'true'
                # Also tag it with kubernetes.io/cluster/clustername=owned, which is the newer convention for cluster resources
              - Key: !Sub 'kubernetes.io/cluster/${ClusterName}'
                Value: 'owned'
                # PropagateAtLaunch: 'true'     
          KeyName: !Ref KeyName
            # !If 
            #   - HasSshKeyName
            #   - !Ref SshKeyName
            #   - !Ref AWS::NoValue
          Monitoring:
            Enabled: 'true'
          NetworkInterfaces:
            - AssociatePublicIpAddress: true
              DeviceIndex: 0
              SubnetId: !Ref Subnet01
              Groups:
                - !Ref NodeSecurityGroup
          UserData: 
            Fn::Base64:
              Fn::Join: [
                "",
                [
                  "#!/bin/bash -x\n",
                  "INTERNAL_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /var/lib/kubelet/kubeconfig", "\n",
                  "sed -i s,CLUSTER_NAME,", { Ref: ClusterName }, ",g /var/lib/kubelet/kubeconfig", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kubelet.service", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kube-proxy.service", "\n",
                  "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kubelet.service", "\n",
                  "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kube-proxy.service", "\n",
                  "systemctl daemon-reload", "\n",
                  "systemctl restart kubelet kube-proxy", "\n",
                  "echo 'start' > /tmp/userdata.log", "\n",
                  "restart_if_fail() { sleep 10; journalctl -u kubelet --since '10 seconds ago' -r -n1 | egrep '[E|W]0401' >> /tmp/userdata.log && echo 'got failure, restarting kubelet and kube-proxy now' | tee -a /tmp/userdata.log && systemctl restart kubelet kube-proxy; }", "\n",
                  "for i in 1 2 3 4 5; do restart_if_fail; done", "\n",   
                ]
              ]
        - EbsOptimized: 'false'
          WeightedCapacity: 1
          IamInstanceProfile: 
             Arn: !GetAtt NodeInstanceProfile.Arn
          InstanceType: !Select [2, !Ref SpotFleetInstancesType]
          # ImageId: !GetAtt AMIInfo.Id
          ImageId: !Ref NodeImageId
          TagSpecifications:
          -
            ResourceType: instance
            Tags:
              - Key: Name
                Value: !Sub "${ClusterName}-${NodeGroupName}-Node"
                # PropagateAtLaunch: 'true'
              - Key: KubernetesCluster
                Value: !Ref ClusterName
                # PropagateAtLaunch: 'true'
                # Also tag it with kubernetes.io/cluster/clustername=owned, which is the newer convention for cluster resources
              - Key: !Sub 'kubernetes.io/cluster/${ClusterName}'
                Value: 'owned'
                # PropagateAtLaunch: 'true'     
          KeyName: !Ref KeyName
            # !If 
            #   - HasSshKeyName
            #   - !Ref SshKeyName
            #   - !Ref AWS::NoValue
          Monitoring:
            Enabled: 'true'
          NetworkInterfaces:
            - AssociatePublicIpAddress: true
              DeviceIndex: 0
              SubnetId: !Ref Subnet02
              Groups:
                - !Ref NodeSecurityGroup
          UserData: 
            Fn::Base64:
              Fn::Join: [
                "",
                [
                  "#!/bin/bash -x\n",
                  "INTERNAL_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /var/lib/kubelet/kubeconfig", "\n",
                  "sed -i s,CLUSTER_NAME,", { Ref: ClusterName }, ",g /var/lib/kubelet/kubeconfig", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kubelet.service", "\n",
                  "sed -i s,MASTER_ENDPOINT,", { Ref: MasterEndpoint }, ",g /etc/systemd/system/kube-proxy.service", "\n",
                  "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kubelet.service", "\n",
                  "sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kube-proxy.service", "\n",
                  "systemctl daemon-reload", "\n",
                  "systemctl restart kubelet kube-proxy", "\n",
                  "echo 'start' > /tmp/userdata.log", "\n",
                  "restart_if_fail() { sleep 10; journalctl -u kubelet --since '10 seconds ago' -r -n1 | egrep '[E|W]0401' >> /tmp/userdata.log && echo 'got failure, restarting kubelet and kube-proxy now' | tee -a /tmp/userdata.log && systemctl restart kubelet kube-proxy; }", "\n",
                  "for i in 1 2 3 4 5; do restart_if_fail; done", "\n",                   
                ]
              ]

Outputs:
  NodeInstanceRole:
    Description: The node instance role
    Value: !GetAtt NodeInstanceRole.Arn